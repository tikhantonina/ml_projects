{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Embeddings\n\nПривет! В этом домашнем задании мы с помощью эмбеддингов решим задачу семантической классификации твитов.\n\nДля этого мы воспользуемся предобученными эмбеддингами word2vec.","metadata":{"id":"eYtJxkhKpYK2"}},{"cell_type":"markdown","source":"Для начала скачаем датасет для семантической классификации твитов:","metadata":{"id":"jBOdoFS8AdpP"}},{"cell_type":"code","source":"# !gdown https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download\n# !unzip archive.zip","metadata":{"id":"wXjhtsfF_gBK","execution":{"iopub.status.busy":"2022-05-02T09:26:56.966985Z","iopub.execute_input":"2022-05-02T09:26:56.967296Z","iopub.status.idle":"2022-05-02T09:26:56.990863Z","shell.execute_reply.started":"2022-05-02T09:26:56.967221Z","shell.execute_reply":"2022-05-02T09:26:56.990192Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Импортируем нужные библиотеки:","metadata":{"id":"Sh6wW-K53Mle"}},{"cell_type":"code","source":"import math\nimport random\nimport string\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport torch\nimport nltk\nimport gensim\nimport gensim.downloader as api\nfrom torch import nn\nimport torch.optim as optim\nwnl = nltk.WordNetLemmatizer()\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopWords = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nnltk.download('punkt')\nnltk.download('wordnet')","metadata":{"id":"A2Y5CHRm6NFe","execution":{"iopub.status.busy":"2022-05-02T09:26:58.377907Z","iopub.execute_input":"2022-05-02T09:26:58.378160Z","iopub.status.idle":"2022-05-02T09:27:01.989263Z","shell.execute_reply.started":"2022-05-02T09:26:58.378132Z","shell.execute_reply":"2022-05-02T09:27:01.988542Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"random.seed(42)\nnp.random.seed(42)\ntorch.random.manual_seed(42)\ntorch.cuda.random.manual_seed(42)\ntorch.cuda.random.manual_seed_all(42)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"73Lb0wbESrgQ","execution":{"iopub.status.busy":"2022-05-02T09:27:01.991135Z","iopub.execute_input":"2022-05-02T09:27:01.991613Z","iopub.status.idle":"2022-05-02T09:27:02.061569Z","shell.execute_reply.started":"2022-05-02T09:27:01.991577Z","shell.execute_reply":"2022-05-02T09:27:02.060868Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T09:27:02.062938Z","iopub.execute_input":"2022-05-02T09:27:02.063181Z","iopub.status.idle":"2022-05-02T09:27:02.074808Z","shell.execute_reply.started":"2022-05-02T09:27:02.063148Z","shell.execute_reply":"2022-05-02T09:27:02.073876Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/archive/training.1600000.processed.noemoticon.csv\", encoding=\"latin\", header=None, names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])","metadata":{"id":"L_Wv-4bu83Fl","execution":{"iopub.status.busy":"2022-05-02T09:27:05.995870Z","iopub.execute_input":"2022-05-02T09:27:05.996121Z","iopub.status.idle":"2022-05-02T09:27:11.288181Z","shell.execute_reply.started":"2022-05-02T09:27:05.996093Z","shell.execute_reply":"2022-05-02T09:27:11.287423Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Посмотрим на данные:","metadata":{"id":"RY1pvYDS3Yuj"}},{"cell_type":"code","source":"data.head()","metadata":{"id":"jST2tjgjCTWD","execution":{"iopub.status.busy":"2022-05-02T09:27:11.289687Z","iopub.execute_input":"2022-05-02T09:27:11.289922Z","iopub.status.idle":"2022-05-02T09:27:11.306449Z","shell.execute_reply.started":"2022-05-02T09:27:11.289888Z","shell.execute_reply":"2022-05-02T09:27:11.305706Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:","metadata":{"id":"OhbR5JJyA2VW"}},{"cell_type":"code","source":"examples = data[\"text\"].sample(10)\nprint(\"\\n\".join(examples))","metadata":{"id":"kCBwe0wR83C2","execution":{"iopub.status.busy":"2022-05-02T09:27:14.896717Z","iopub.execute_input":"2022-05-02T09:27:14.897253Z","iopub.status.idle":"2022-05-02T09:27:14.952302Z","shell.execute_reply.started":"2022-05-02T09:27:14.897218Z","shell.execute_reply":"2022-05-02T09:27:14.951552Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Как виим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n\nЧтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели).","metadata":{"id":"GvcYW8aX3mKt"}},{"cell_type":"code","source":"indexes = np.arange(data.shape[0])\nnp.random.shuffle(indexes)\ndev_size = math.ceil(data.shape[0] * 0.8)\n\ndev_indexes = indexes[:dev_size]\ntest_indexes = indexes[dev_size:]\n\ndev_data = data.iloc[dev_indexes]\ntest_data = data.iloc[test_indexes]\n\ndev_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)","metadata":{"id":"f8hUK-jnQg6O","execution":{"iopub.status.busy":"2022-05-02T09:27:17.279046Z","iopub.execute_input":"2022-05-02T09:27:17.279580Z","iopub.status.idle":"2022-05-02T09:27:18.431676Z","shell.execute_reply.started":"2022-05-02T09:27:17.279547Z","shell.execute_reply":"2022-05-02T09:27:18.430947Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Обработка текста","metadata":{"id":"6ivcpeFoCnZA"}},{"cell_type":"markdown","source":"Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:","metadata":{"id":"Df4nca285Dar"}},{"cell_type":"code","source":"tokenizer = nltk.WordPunctTokenizer()\nline = tokenizer.tokenize(dev_data[\"text\"][0].lower())\nprint(\" \".join(line))\n# print(len(line))","metadata":{"id":"nsNHNDES9ZVF","execution":{"iopub.status.busy":"2022-05-02T09:27:19.667583Z","iopub.execute_input":"2022-05-02T09:27:19.668376Z","iopub.status.idle":"2022-05-02T09:27:19.675015Z","shell.execute_reply.started":"2022-05-02T09:27:19.668324Z","shell.execute_reply":"2022-05-02T09:27:19.674312Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\nprint(\" \".join(filtered_line))\nprint(len(filtered_line))","metadata":{"id":"GcBS_u_hTuxp","execution":{"iopub.status.busy":"2022-05-02T09:27:20.892610Z","iopub.execute_input":"2022-05-02T09:27:20.893127Z","iopub.status.idle":"2022-05-02T09:27:20.898868Z","shell.execute_reply.started":"2022-05-02T09:27:20.893092Z","shell.execute_reply":"2022-05-02T09:27:20.897771Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Загрузим предобученную модель эмбеддингов. \n\nЕсли хотите, можно попробовать другую. Полный список можно найти здесь: https://github.com/RaRe-Technologies/gensim-data.\n\nДанная модель выдает эмбеддинги для **слов**. Строить по эмбеддингам слов эмбеддинги **предложений** мы будем ниже.","metadata":{"id":"cuFmlXkC6E7X"}},{"cell_type":"code","source":"word2vec = api.load(\"glove-wiki-gigaword-50\") #api.load(\"glove-wiki-gigaword-50\") #api.load(\"word2vec-google-news-300\")","metadata":{"id":"cACJpje2T5bc","execution":{"iopub.status.busy":"2022-05-02T10:16:28.002950Z","iopub.execute_input":"2022-05-02T10:16:28.003462Z","iopub.status.idle":"2022-05-02T10:16:47.895429Z","shell.execute_reply.started":"2022-05-02T10:16:28.003427Z","shell.execute_reply":"2022-05-02T10:16:47.894525Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\nprint(sum(emb_line).shape)\n# print(emb_line)","metadata":{"id":"NafmYHrkT5YD","execution":{"iopub.status.busy":"2022-05-02T10:16:57.980509Z","iopub.execute_input":"2022-05-02T10:16:57.981028Z","iopub.status.idle":"2022-05-02T10:16:58.024361Z","shell.execute_reply.started":"2022-05-02T10:16:57.980990Z","shell.execute_reply":"2022-05-02T10:16:58.023463Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Нормализуем эмбеддинги, прежде чем обучать на них сеть. \n(наверное, вы помните, что нейронные сети гораздо лучше обучаются на нормализованных данных)","metadata":{"id":"LTS6LCkd6_E7"}},{"cell_type":"code","source":"mean = np.mean(word2vec.vectors, 0)\nstd = np.std(word2vec.vectors, 0)\nnorm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n# print(sum(norm_emb_line))\nprint(sum(norm_emb_line).shape)\nprint([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])\nprint(len(norm_emb_line))","metadata":{"id":"3PyLTZ6xf3Oq","execution":{"iopub.status.busy":"2022-05-02T10:17:24.054778Z","iopub.execute_input":"2022-05-02T10:17:24.055306Z","iopub.status.idle":"2022-05-02T10:17:24.147923Z","shell.execute_reply.started":"2022-05-02T10:17:24.055267Z","shell.execute_reply":"2022-05-02T10:17:24.147067Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Сделаем датасет, который будет по запросу возвращать подготовленные данные.","metadata":{"id":"q7vm6Ppd7Ubw"}},{"cell_type":"code","source":"from torch.utils.data import Dataset, random_split\n\n\nclass TwitterDataset(Dataset):\n    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n        self.tokenizer = nltk.WordPunctTokenizer()\n        \n        self.data = data\n\n        self.feature_column = feature_column\n        self.target_column = target_column\n\n        self.word2vec = word2vec\n\n        self.label2num = lambda label: 0 if label == 0 else 1\n        self.mean = np.mean(word2vec.vectors, axis=0)\n        self.std = np.std(word2vec.vectors, axis=0)\n\n    def __getitem__(self, item):\n        text = self.data[self.feature_column][item]\n        label = self.label2num(self.data[self.target_column][item])\n\n        tokens = self.get_tokens_(text)\n        embeddings = self.get_embeddings_(tokens)\n        return {\"feature\": embeddings, \"target\": label}\n\n    def get_tokens_(self, text):\n        line = tokenizer.tokenize(text.lower())\n        if line[0] == '@': line[1] = '@'\n        line = ' '.join(w for w in line if all(c not in string.punctuation for c in w))\n\n        tokens = ' '.join(wnl.lemmatize(word) for word in word_tokenize(text.lower())  if word not in stopWords\n                     )\n        return tokens\n\n    def get_embeddings_(self, tokens):\n\n        embeddings = np.zeros((1, self.word2vec.vector_size))\n        \n        k = 0\n        for w in tokens.split():\n          if w in word2vec:\n            embeddings = embeddings + word2vec.get_vector(w)\n            k += 1\n\n        if k != 0:\n          embeddings = (embeddings/k- self.mean)/ self.std \n\n        if len(embeddings) == 0:\n            embeddings = np.zeros((1, self.word2vec.vector_size))\n        else:\n            embeddings = np.array(embeddings)\n            if len(embeddings.shape) == 1:\n                embeddings = embeddings.reshape(-1, 1)\n        return embeddings\n\n    def __len__(self):\n        return self.data.shape[0]","metadata":{"id":"b4eZajF7pZ1X","execution":{"iopub.status.busy":"2022-05-02T10:17:26.697382Z","iopub.execute_input":"2022-05-02T10:17:26.697705Z","iopub.status.idle":"2022-05-02T10:17:26.722716Z","shell.execute_reply.started":"2022-05-02T10:17:26.697668Z","shell.execute_reply":"2022-05-02T10:17:26.721786Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)","metadata":{"id":"IZJpttbXpZyz","execution":{"iopub.status.busy":"2022-05-02T10:17:27.725939Z","iopub.execute_input":"2022-05-02T10:17:27.726527Z","iopub.status.idle":"2022-05-02T10:17:27.816023Z","shell.execute_reply.started":"2022-05-02T10:17:27.726460Z","shell.execute_reply":"2022-05-02T10:17:27.815127Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"indexes = np.arange(len(dev))\nnp.random.shuffle(indexes)\nexample_indexes = indexes[::1000]\n\nexamples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\nprint(len(examples[\"features\"]))\nprint(np.asarray(examples[\"features\"]).shape)","metadata":{"id":"_OsVz4c82MAx","execution":{"iopub.status.busy":"2022-05-02T10:17:33.955410Z","iopub.execute_input":"2022-05-02T10:17:33.956108Z","iopub.status.idle":"2022-05-02T10:17:37.190425Z","shell.execute_reply.started":"2022-05-02T10:17:33.956075Z","shell.execute_reply":"2022-05-02T10:17:37.189543Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Отлично, мы готовы с помощью эмбеддингов слов превращать твиты в векторы и обучать нейронную сеть.\n\nПревращать твиты в векторы, используя эмбеддинги слов, можно несколькими способами. А именно такими:","metadata":{"id":"Sr-aetH0_LH1"}},{"cell_type":"markdown","source":"## Average embedding (2 балла)\n---\nЭто самый простой вариант, как получить вектор предложения, используя векторные представления слов в предложении. А именно: вектор предложения есть средний вектор всех слов в предлоежнии (которые остались после токенизации и удаления коротких слов, конечно). ","metadata":{"id":"4AhHrWa196Yc"}},{"cell_type":"code","source":"indexes = np.arange(len(dev))\nnp.random.shuffle(indexes)\nexample_indexes = indexes[::1000]\n\nexamples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\nprint(len(examples[\"features\"]))\nprint(np.asarray(examples[\"features\"]).shape)","metadata":{"id":"ScdokSW-994t","execution":{"iopub.status.busy":"2022-05-02T10:17:38.847658Z","iopub.execute_input":"2022-05-02T10:17:38.848186Z","iopub.status.idle":"2022-05-02T10:17:39.971663Z","shell.execute_reply.started":"2022-05-02T10:17:38.848149Z","shell.execute_reply":"2022-05-02T10:17:39.970922Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета. Так мы увидим, насколько хорошо твиты с разными target значениями отделяются друг от друга, т.е. насколько хорошо усреднение эмбеддингов слов предложения передает информацию о предложении.","metadata":{"id":"1yGQ_lOx_1NL"}},{"cell_type":"markdown","source":"Для визуализации векторов надо получить их проекцию на плоскость. Сделаем это с помощью `PCA`. Если хотите, можете вместо PCA использовать TSNE: так у вас получится более точная проекция на плоскость (а значит, более информативная, т.е. отражающая реальное положение векторов твитов в пространстве). Но TSNE будет работать намного дольше.","metadata":{"id":"LZwFksd_8uYO"}},{"cell_type":"code","source":"print(np.asarray(examples['features']).shape)","metadata":{"id":"KEk4s0IWsdK9","execution":{"iopub.status.busy":"2022-05-02T10:17:39.973198Z","iopub.execute_input":"2022-05-02T10:17:39.973612Z","iopub.status.idle":"2022-05-02T10:17:39.979376Z","shell.execute_reply.started":"2022-05-02T10:17:39.973575Z","shell.execute_reply":"2022-05-02T10:17:39.978713Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n\npca = PCA(n_components=2)\nexamples[\"transformed_features\"] = pca.fit_transform(np.asarray(examples['features']))\n# Обучи PCA на эмбеддингах слов","metadata":{"id":"aKFZRSHdtIac","execution":{"iopub.status.busy":"2022-05-02T10:17:40.118719Z","iopub.execute_input":"2022-05-02T10:17:40.118907Z","iopub.status.idle":"2022-05-02T10:17:40.173563Z","shell.execute_reply.started":"2022-05-02T10:17:40.118884Z","shell.execute_reply":"2022-05-02T10:17:40.172873Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(np.asarray(examples['features']).shape)","metadata":{"id":"Bl1CAMFe_juM","execution":{"iopub.status.busy":"2022-05-02T10:17:40.312696Z","iopub.execute_input":"2022-05-02T10:17:40.312886Z","iopub.status.idle":"2022-05-02T10:17:40.319272Z","shell.execute_reply.started":"2022-05-02T10:17:40.312863Z","shell.execute_reply":"2022-05-02T10:17:40.318596Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(examples[\"transformed_features\"].shape)","metadata":{"id":"2Uf5c2YqvNnc","execution":{"iopub.status.busy":"2022-05-02T10:17:41.065008Z","iopub.execute_input":"2022-05-02T10:17:41.065835Z","iopub.status.idle":"2022-05-02T10:17:41.070691Z","shell.execute_reply.started":"2022-05-02T10:17:41.065789Z","shell.execute_reply":"2022-05-02T10:17:41.069908Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(np.sum(np.asarray(examples['features'])))\nprint(sum(examples[\"transformed_features\"]))\nprint(np.sum(examples[\"targets\"]))","metadata":{"id":"Ez5VdK2Ii02n","execution":{"iopub.status.busy":"2022-05-02T10:17:41.241781Z","iopub.execute_input":"2022-05-02T10:17:41.242073Z","iopub.status.idle":"2022-05-02T10:17:41.252248Z","shell.execute_reply.started":"2022-05-02T10:17:41.242043Z","shell.execute_reply":"2022-05-02T10:17:41.251443Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import bokeh.models as bm, bokeh.plotting as pl\nfrom bokeh.io import output_notebook\noutput_notebook()\n\ndef draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n                 width=600, height=400, show=True, **kwargs):\n    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n\n    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n\n    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n    if show: pl.show(fig)\n    return fig","metadata":{"id":"szEOWdiNtIX8","execution":{"iopub.status.busy":"2022-05-02T10:17:41.462191Z","iopub.execute_input":"2022-05-02T10:17:41.462586Z","iopub.status.idle":"2022-05-02T10:17:41.827643Z","shell.execute_reply.started":"2022-05-02T10:17:41.462551Z","shell.execute_reply":"2022-05-02T10:17:41.827005Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print(sum(examples[\"transformed_features\"]))","metadata":{"id":"HCBSAZ4qvB0B","execution":{"iopub.status.busy":"2022-05-02T10:17:41.829215Z","iopub.execute_input":"2022-05-02T10:17:41.829608Z","iopub.status.idle":"2022-05-02T10:17:41.837093Z","shell.execute_reply.started":"2022-05-02T10:17:41.829569Z","shell.execute_reply":"2022-05-02T10:17:41.836155Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"draw_vectors(\n    examples[\"transformed_features\"][:, 0], \n    examples[\"transformed_features\"][:, 1], \n    color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]]\n    )","metadata":{"id":"7OONK8ldtIWe","execution":{"iopub.status.busy":"2022-05-02T10:17:41.855954Z","iopub.execute_input":"2022-05-02T10:17:41.856227Z","iopub.status.idle":"2022-05-02T10:17:41.924105Z","shell.execute_reply.started":"2022-05-02T10:17:41.856200Z","shell.execute_reply":"2022-05-02T10:17:41.923515Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Скорее всего, на визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n\nПодготовим загрузчики данных.\nУсреднее векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n","metadata":{"id":"0fNF6LRQ9MPI"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n\nbatch_size = 300\nnum_workers = 2\n\ndef average_emb(batch):\n    features = [np.mean(b[\"feature\"], axis=0) for b in batch]\n    targets = [b[\"target\"] for b in batch]\n\n    return {\"features\": torch.FloatTensor(features), \"targets\": torch.LongTensor(targets)}\n\n\ntrain_size = math.ceil(len(dev) * 0.8)\n\ntrain, valid = random_split(dev, [train_size, len(dev) - train_size])\n\ntrain_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\nvalid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)","metadata":{"id":"y1XapsADtITv","execution":{"iopub.status.busy":"2022-05-02T10:17:42.202676Z","iopub.execute_input":"2022-05-02T10:17:42.202968Z","iopub.status.idle":"2022-05-02T10:17:42.325126Z","shell.execute_reply.started":"2022-05-02T10:17:42.202939Z","shell.execute_reply":"2022-05-02T10:17:42.324346Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Определим функции для тренировки и теста модели:","metadata":{"id":"p-zs0WEK-Vkt"}},{"cell_type":"code","source":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc","metadata":{"id":"vmowlGSlIsV0","execution":{"iopub.status.busy":"2022-05-02T10:17:43.477778Z","iopub.execute_input":"2022-05-02T10:17:43.478039Z","iopub.status.idle":"2022-05-02T10:17:43.483815Z","shell.execute_reply.started":"2022-05-02T10:17:43.478005Z","shell.execute_reply":"2022-05-02T10:17:43.482750Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\n\ndef training(model, optimizer, criterion, train_loader, epoch, device=\"cpu\"):\n    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}\")\n    model.train()\n    for batch in pbar:\n\n        targets = batch['targets'].to(device)\n        features = batch['features'].to(device)\n\n        # Получи предсказания модели\n        predictions = model(features)\n        predictions = predictions.view(predictions.size(0))\n\n        loss = criterion(predictions, targets.float())\n        acc = binary_acc(predictions, targets.float())\n\n        # Посчитай лосс\n        # Обнови параметры модели\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}, Test Acc: {acc:.4}\")\n    \n\ndef testing(model, criterion, test_loader, device=\"cpu\"):\n    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n    mean_loss = 0\n    mean_acc = 0\n    model.eval()\n    with torch.no_grad():\n        for batch in pbar:\n            targets = batch['targets'].to(device)\n            features = batch['features'].to(device)\n\n            \n            predictions = model(features)\n            predictions = predictions.view(predictions.size(0))\n\n            loss = criterion(predictions, targets.float())\n            acc = binary_acc(predictions, targets.float())\n\n            mean_loss += loss.item()\n            mean_acc += acc.item()\n\n            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n\n    pbar.set_description(f\"Test Loss: {mean_loss / len(test_loader):.4}, Test Acc: {mean_acc / len(test_loader):.4}\")\n\n    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}\n","metadata":{"id":"U--T2Gjw1r27","execution":{"iopub.status.busy":"2022-05-02T10:17:44.298443Z","iopub.execute_input":"2022-05-02T10:17:44.299045Z","iopub.status.idle":"2022-05-02T10:17:44.322447Z","shell.execute_reply.started":"2022-05-02T10:17:44.299009Z","shell.execute_reply":"2022-05-02T10:17:44.321741Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Создадим модель, оптимизатор и целевую функцию. Вы можете сами выбрать количество слоев в нейронной сети, ваш любимый оптимизатор и целевую функцию.\n","metadata":{"id":"oVg_XBBb-YBH"}},{"cell_type":"code","source":"class BinaryClassification(nn.Module):\n    def __init__(self):\n        super(BinaryClassification, self).__init__()\n        # Number of input features 200\n        hid_size = 50\n        self.layer_1 = nn.Linear(hid_size, hid_size*2) \n        self.layer_2 = nn.Linear(hid_size*2, hid_size*2)\n        self.layer_out = nn.Linear(hid_size*2, 1) \n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(hid_size*2)\n        self.batchnorm2 = nn.BatchNorm1d(hid_size*2)\n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.layer_out(x)\n        \n        return x","metadata":{"id":"yoOU-Ouu57SP","execution":{"iopub.status.busy":"2022-05-02T10:17:45.615271Z","iopub.execute_input":"2022-05-02T10:17:45.615547Z","iopub.status.idle":"2022-05-02T10:17:45.623494Z","shell.execute_reply.started":"2022-05-02T10:17:45.615506Z","shell.execute_reply":"2022-05-02T10:17:45.622605Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"y1oZRVPaA6Nt","execution":{"iopub.status.busy":"2022-05-02T10:17:46.179212Z","iopub.execute_input":"2022-05-02T10:17:46.179696Z","iopub.status.idle":"2022-05-02T10:17:46.186557Z","shell.execute_reply.started":"2022-05-02T10:17:46.179656Z","shell.execute_reply":"2022-05-02T10:17:46.185400Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model = BinaryClassification()\nmodel.to(device)\nprint(model)\ncriterion = nn.BCEWithLogitsLoss()  #nn.BCEWithLogitsLoss(), nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())","metadata":{"id":"CMA-OeccA6VQ","execution":{"iopub.status.busy":"2022-05-02T10:17:46.556231Z","iopub.execute_input":"2022-05-02T10:17:46.556551Z","iopub.status.idle":"2022-05-02T10:17:49.641616Z","shell.execute_reply.started":"2022-05-02T10:17:46.556524Z","shell.execute_reply":"2022-05-02T10:17:49.640874Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Наконец, обучим модель и протестируем её.\n\nПосле каждой эпохи будем проверять качество модели на валидационной части датасета. Если метрика стала лучше, будем сохранять модель. **Подумайте, какая метрика (точность или лосс) будет лучше работать в этой задаче?** ","metadata":{"id":"-AitU8AR-zBj"}},{"cell_type":"code","source":"best_metric = np.inf\nnum_epochs = 6\nfor e in range(num_epochs):\n    training(model, optimizer, criterion, train_loader, e, device)\n    log = testing(model, criterion, valid_loader, device)\n    print(log)\n    if log[\"Test Loss\"] < best_metric:\n        torch.save(model.state_dict(), \"model.pt\")\n        best_metric = log[\"Test Loss\"]","metadata":{"id":"j2pTj-FWA6W7","execution":{"iopub.status.busy":"2022-05-02T10:17:55.163342Z","iopub.execute_input":"2022-05-02T10:17:55.163617Z","iopub.status.idle":"2022-05-02T11:07:55.501191Z","shell.execute_reply.started":"2022-05-02T10:17:55.163587Z","shell.execute_reply":"2022-05-02T11:07:55.500220Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Embeddings for unknown words (8 баллов)\n\nПока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n\n- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста. (4 балла)\n- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf. (4 балла)\n\nРеализуйте оба варианта **ниже**. Напишите, какой способ сработал лучше и ваши мысли, почему так получилось.","metadata":{"id":"ZRvzpldHSAu0"}},{"cell_type":"markdown","source":"## Модель с предобученными эмбедингами + Tfidf\n\nДля каждого слова текста получим его эмбеддинг из Tfidf с помощью TfidfVectorizer из sklearn. Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf.","metadata":{"id":"7iY0y8GIZjhf"}},{"cell_type":"code","source":"import spacy \nnlp = spacy.load(\"en_core_web_sm\")\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport matplotlib.pyplot as plt","metadata":{"id":"MTJnxdzf2Udc","execution":{"iopub.status.busy":"2022-05-02T11:07:55.504274Z","iopub.execute_input":"2022-05-02T11:07:55.505112Z","iopub.status.idle":"2022-05-02T11:08:04.592763Z","shell.execute_reply.started":"2022-05-02T11:07:55.505067Z","shell.execute_reply":"2022-05-02T11:08:04.592006Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"tokenizer = nltk.WordPunctTokenizer()\n\nvectorizer = TfidfVectorizer(analyzer=\"word\", tokenizer=nltk.word_tokenize,\n    preprocessor=None, stop_words='english', max_features=None)\ntfidf = vectorizer.fit_transform(dev_data['text'])    \ndictionary = vectorizer.get_feature_names_out()","metadata":{"id":"hVCUQywskgqk","execution":{"iopub.status.busy":"2022-05-02T12:08:38.347903Z","iopub.execute_input":"2022-05-02T12:08:38.348137Z","iopub.status.idle":"2022-05-02T12:14:10.216463Z","shell.execute_reply.started":"2022-05-02T12:08:38.348104Z","shell.execute_reply":"2022-05-02T12:14:10.215722Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"dict_tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))","metadata":{"id":"zH6YReCK5EAW","execution":{"iopub.status.busy":"2022-05-02T12:14:10.217732Z","iopub.execute_input":"2022-05-02T12:14:10.218062Z","iopub.status.idle":"2022-05-02T12:14:11.537652Z","shell.execute_reply.started":"2022-05-02T12:14:10.218026Z","shell.execute_reply":"2022-05-02T12:14:11.536876Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"maximum = max(dict_tfidf.values())","metadata":{"id":"DDMomzIqf_jZ","execution":{"iopub.status.busy":"2022-05-02T12:14:11.539515Z","iopub.execute_input":"2022-05-02T12:14:11.539759Z","iopub.status.idle":"2022-05-02T12:14:11.588120Z","shell.execute_reply.started":"2022-05-02T12:14:11.539726Z","shell.execute_reply":"2022-05-02T12:14:11.587288Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"dict_tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_/maximum))","metadata":{"id":"e0XZ_eG1gNpv","execution":{"iopub.status.busy":"2022-05-02T12:14:11.589451Z","iopub.execute_input":"2022-05-02T12:14:11.590140Z","iopub.status.idle":"2022-05-02T12:14:12.749967Z","shell.execute_reply.started":"2022-05-02T12:14:11.590101Z","shell.execute_reply":"2022-05-02T12:14:12.749108Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"class TwitterDataset_tfidf(Dataset):\n    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, \n                 word2vec: gensim.models.Word2Vec, dict_tfidf : dict):\n        self.tokenizer = nltk.WordPunctTokenizer()\n        \n        self.data = data\n\n        self.feature_column = feature_column\n        self.target_column = target_column\n\n        self.word2vec = word2vec\n        self.dict_tfidf = dict_tfidf\n\n        self.label2num = lambda label: 0 if label == 0 else 1\n        self.mean = np.mean(word2vec.vectors, axis=0)\n        self.std = np.std(word2vec.vectors, axis=0)\n        \n\n\n    def __getitem__(self, item):\n        text = self.data[self.feature_column][item]\n        label = self.label2num(self.data[self.target_column][item])\n\n        tokens = self.get_tokens_(text)\n        embeddings = self.get_embeddings_(tokens)\n        \n        return {\"feature\": embeddings, \"target\": label}\n\n    def get_tokens_(self, text):\n        line = tokenizer.tokenize(text.lower())\n        if line[0] == '@': line[1] = '@'\n        line = ' '.join(w for w in line if all(c not in string.punctuation for c in w))\n\n        tokens = ' '.join(wnl.lemmatize(word) for word in word_tokenize(text.lower())  if word not in stopWords\n                     )\n        return tokens\n\n    def get_embeddings_(self, tokens):\n\n        embeddings = np.zeros((1, self.word2vec.vector_size))\n        embeddings_tfidf = np.zeros((1, self.word2vec.vector_size))\n        \n        k = 0\n        j = 0\n        for w in tokens.split():\n          if w in word2vec:\n            embeddings += word2vec.get_vector(w)\n            k += 1\n          if w in dict_tfidf:\n            embeddings_tfidf += np.ones(word2vec.vector_size)*dict_tfidf[w]\n            j += 1\n\n        if k != 0:\n          embeddings = (embeddings/k - self.mean)/ self.std\n        if j != 0:\n          embeddings += embeddings_tfidf/j\n        if j != 0 and k != 0:\n          embeddings = embeddings/2  \n\n        if len(embeddings) == 0:\n            embeddings = np.zeros((1, self.word2vec.vector_size))\n        else:\n            embeddings = np.array(embeddings)\n            if len(embeddings.shape) == 1:\n                embeddings = embeddings.reshape(-1, 1)\n        \n        return embeddings\n\n    def __len__(self):\n        return self.data.shape[0]","metadata":{"id":"f3rTuXhMV6CQ","execution":{"iopub.status.busy":"2022-05-02T12:14:12.751390Z","iopub.execute_input":"2022-05-02T12:14:12.751913Z","iopub.status.idle":"2022-05-02T12:14:12.780247Z","shell.execute_reply.started":"2022-05-02T12:14:12.751876Z","shell.execute_reply":"2022-05-02T12:14:12.774591Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"dev = TwitterDataset_tfidf(dev_data, \"text\", \"emotion\", word2vec, dict_tfidf)","metadata":{"id":"XfVV2lhUW8JI","execution":{"iopub.status.busy":"2022-05-02T12:14:12.782357Z","iopub.execute_input":"2022-05-02T12:14:12.784138Z","iopub.status.idle":"2022-05-02T12:14:12.917929Z","shell.execute_reply.started":"2022-05-02T12:14:12.783983Z","shell.execute_reply":"2022-05-02T12:14:12.917137Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"indexes = np.arange(len(dev))\nnp.random.shuffle(indexes)\nexample_indexes = indexes[::1000]\n\nexamples = {\"features\": [np.sum(dev[i][\"feature\"], axis=0) for i in example_indexes], \n            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\nprint(len(examples[\"features\"]))\nprint(np.asarray(examples[\"features\"]).shape)","metadata":{"id":"74qDoOjlW8Lx","execution":{"iopub.status.busy":"2022-05-02T12:14:12.919111Z","iopub.execute_input":"2022-05-02T12:14:12.919514Z","iopub.status.idle":"2022-05-02T12:14:14.458809Z","shell.execute_reply.started":"2022-05-02T12:14:12.919462Z","shell.execute_reply":"2022-05-02T12:14:14.457357Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"model_tfidf = BinaryClassification()\nmodel_tfidf.to(device)\nprint(model_tfidf)","metadata":{"id":"BxAue7p-oLuf","execution":{"iopub.status.busy":"2022-05-02T12:14:14.460057Z","iopub.execute_input":"2022-05-02T12:14:14.460304Z","iopub.status.idle":"2022-05-02T12:14:14.471029Z","shell.execute_reply.started":"2022-05-02T12:14:14.460269Z","shell.execute_reply":"2022-05-02T12:14:14.470238Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"batch_size = 300\nnum_workers = 2\n\ntrain_size = math.ceil(len(dev) * 0.8)\n\ntrain, valid = random_split(dev, [train_size, len(dev) - train_size])\n\ntrain_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\nvalid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)","metadata":{"id":"5cBOAr32ZKIY","execution":{"iopub.status.busy":"2022-05-02T12:14:14.474413Z","iopub.execute_input":"2022-05-02T12:14:14.474628Z","iopub.status.idle":"2022-05-02T12:14:14.648053Z","shell.execute_reply.started":"2022-05-02T12:14:14.474603Z","shell.execute_reply":"2022-05-02T12:14:14.647316Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"best_metric = np.inf\nnum_epochs = 6\nfor e in range(num_epochs):\n    training(model_tfidf, optimizer, criterion, train_loader, e, device)\n    log = testing(model_tfidf, criterion, valid_loader, device)\n    print(log)\n    if log[\"Test Loss\"] < best_metric:\n        torch.save(model.state_dict(), \"model.pt\")\n        best_metric = log[\"Test Loss\"]","metadata":{"id":"e44iqG3xZPfu","execution":{"iopub.status.busy":"2022-05-02T12:14:14.649423Z","iopub.execute_input":"2022-05-02T12:14:14.649697Z","iopub.status.idle":"2022-05-02T13:16:03.412688Z","shell.execute_reply.started":"2022-05-02T12:14:14.649662Z","shell.execute_reply":"2022-05-02T13:16:03.411759Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"LHbAsuCOZVKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Модель с дополненным по контексту словарем\nСоздадим дополнительно словарь новых слов и добавим его в словарь word2vec.\nВ качестве вектора новых слов берем сумму ближайших слов слева и справа, если они есть в словаре word2vec. В случае, если для слова не нашелся ненулевой вектор для слова, то оно не добавляется в словарь. ","metadata":{"id":"D3yFCxTw9ebR"}},{"cell_type":"code","source":"numb_of_words = dict()\nnew_words = dict()\nvect = []\nfor i in range(len(dev_data['text'])):\n  line = tokenizer.tokenize(dev_data[\"text\"][i].lower())\n  if line[0] == '@': line[1] = '@'\n  line = [w for w in line if all(c not in string.punctuation for c in w)]\n\n  for j in range(len(line)):\n    if line[j] not in word2vec:\n      vect = np.zeros(word2vec.vector_size)\n      for k in range(j, len(line)):\n        if line[k] in word2vec:\n          vect = word2vec.get_vector(line[k])\n          break\n      if line[j] not in numb_of_words:\n          numb_of_words[line[j]] = 0\n      if sum(vect) != 0:\n        numb_of_words[line[j]] += 1\n      new_words[line[j]] = vect\n      for k in range(0, j):\n        if line[j-k] in word2vec:\n          vect_right = word2vec.get_vector(line[j-k])\n          break\n      if sum(vect) != 0:\n        numb_of_words[line[j]] += 1\n      new_words[line[j]] = new_words[line[j]] + vect\n      if sum(new_words[line[j]]) == 0:\n        new_words.pop(line[j])\n        numb_of_words.pop(line[j])","metadata":{"id":"hJ4GJmwE_7wM","execution":{"iopub.status.busy":"2022-05-02T13:16:03.414791Z","iopub.execute_input":"2022-05-02T13:16:03.415346Z","iopub.status.idle":"2022-05-02T13:17:40.400757Z","shell.execute_reply.started":"2022-05-02T13:16:03.415298Z","shell.execute_reply":"2022-05-02T13:17:40.399988Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"for w in new_words:\n  if numb_of_words[w] != 0:\n    new_words[w] = new_words[w]/numb_of_words[w]","metadata":{"id":"G_K_jyzarHEU","execution":{"iopub.status.busy":"2022-05-02T13:17:40.402205Z","iopub.execute_input":"2022-05-02T13:17:40.402451Z","iopub.status.idle":"2022-05-02T13:17:40.841227Z","shell.execute_reply.started":"2022-05-02T13:17:40.402418Z","shell.execute_reply":"2022-05-02T13:17:40.840495Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"word2vec.add_vectors(list(new_words.keys()), list(new_words.values()))","metadata":{"id":"2dNbrQcL8qId","execution":{"iopub.status.busy":"2022-05-02T13:37:47.212759Z","iopub.execute_input":"2022-05-02T13:37:47.213029Z","iopub.status.idle":"2022-05-02T13:37:48.013060Z","shell.execute_reply.started":"2022-05-02T13:37:47.213001Z","shell.execute_reply":"2022-05-02T13:37:48.012340Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)","metadata":{"id":"N0Nr2eFH2xY4","execution":{"iopub.status.busy":"2022-05-02T13:37:51.654983Z","iopub.execute_input":"2022-05-02T13:37:51.655261Z","iopub.status.idle":"2022-05-02T13:37:51.794233Z","shell.execute_reply.started":"2022-05-02T13:37:51.655231Z","shell.execute_reply":"2022-05-02T13:37:51.793218Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"model_add = BinaryClassification()\nmodel_add.to(device)\nprint(model_add)","metadata":{"id":"dh6z6-6wY5tu","execution":{"iopub.status.busy":"2022-05-02T13:37:53.516944Z","iopub.execute_input":"2022-05-02T13:37:53.517189Z","iopub.status.idle":"2022-05-02T13:37:53.526709Z","shell.execute_reply.started":"2022-05-02T13:37:53.517161Z","shell.execute_reply":"2022-05-02T13:37:53.525949Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"batch_size = 300\nnum_workers = 2\n\ntrain_size = math.ceil(len(dev) * 0.8)\n\ntrain, valid = random_split(dev, [train_size, len(dev) - train_size])\n\ntrain_loader = DataLoader(train, batch_size=batch_size, num_workers=num_workers, shuffle=True, drop_last=True, collate_fn=average_emb)\nvalid_loader = DataLoader(valid, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, collate_fn=average_emb)","metadata":{"id":"4kYSHg3v2xbl","execution":{"iopub.status.busy":"2022-05-02T13:37:54.393111Z","iopub.execute_input":"2022-05-02T13:37:54.393790Z","iopub.status.idle":"2022-05-02T13:37:54.508557Z","shell.execute_reply.started":"2022-05-02T13:37:54.393754Z","shell.execute_reply":"2022-05-02T13:37:54.507822Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"best_metric = np.inf\nnum_epochs = 6\nfor e in range(num_epochs):\n    training(model_add, optimizer, criterion, train_loader, e, device)\n    log = testing(model_add, criterion, valid_loader, device)\n    print(log)\n    if log[\"Test Loss\"] < best_metric:\n        torch.save(model.state_dict(), \"model.pt\")\n        best_metric = log[\"Test Loss\"]","metadata":{"id":"7uI4RKOV-jP0","execution":{"iopub.status.busy":"2022-05-02T13:37:55.008751Z","iopub.execute_input":"2022-05-02T13:37:55.009016Z","iopub.status.idle":"2022-05-02T14:28:45.243262Z","shell.execute_reply.started":"2022-05-02T13:37:55.008979Z","shell.execute_reply":"2022-05-02T14:28:45.242330Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}